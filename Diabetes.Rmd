---
title: "DiabetesPrediction"
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

1) All the libraries that have been used during this project

```{r Diabetes}
install.packages('class')
library(ModelMetrics)
library(caret)
library(class)
#library(KODAMA)
library(plyr)
library(BBmisc)
library(rsample)
library(leaps)
library(forecast)
library(ROCR)
library(randomForest)
library(e1071)
library(data.table)
library(foreach)


```


2)Place the csv file in your working directory. This is to avoid giving absolute   paths in your code.
3)Read the file using fread command.(This will return a data frame)
4)Then convert it to DAta Table as it is more efficient than data frame.



```{r}
diab<-fread('diabetes.csv')
diab<- setDT(diab)
```


5) Convert your Outcome variable to a factor.


```{r}
diab$Outcome<- as.factor(diab$Outcome)
```


6) Split the data into training and test data.
7) Train the model on training Data and validate on Test Data
8) In the below code, 70% of the data comes under training data and remaining 30% under Test Data.


```{r}
set.seed(200)
split.obj<-initial_split(diab,prop = .7)
trainingData<- training(split.obj)
trainingData.normalise<- normalize(trainingData,method='standardize')
testData<- testing(split.obj)
testData.normalise<- normalize(testData,method = 'standardize')
```


9)  Now I start modeling the data. At the first instance I have taken all variables as predictors and looking at the results I will start the elimination process. 
10) I first applied Logistic Regression to train Training Data.
11) glm model gives Probability of 'Outcome' being a Yes. i.e. a patient is predicted to      have diabetes.
12) I took 0.5 as the threshold probability for this model
13) I then construct a Confusion Matrix to determine the accuracy of this model.


```{r}

glmModel<- glm(Outcome ~        .,family=binomial(link="logit"),data=trainingData.normalise)
summary(glmModel)
fitted.results <- predict(glmModel,newdata=testData.normalise,type='response')
fitted.results.binary <- ifelse(fitted.results > 0.5,1,0)
fitted.results.binary<-as.factor(fitted.results.binary)
cc<- table(testData.normalise$Outcome,fitted.results.binary)
cc
accuracy<- sum(diag(cc))/sum(cc)
accuracy

#table(testData.normalise$Outcome)

```


14) Looking at the summary(glmModel) we can see that Skin Thickness and Age are     not significant. 
15) Confusion Matrix shows an accuracy of 76.5% which is not good enough.
16) So now I will remodel after eliminationg the non significant variables.
17) So I decided to remove the non significant variable  from my training Data      and Test      Data and remodel.
18) I have also plotted ROC curve for this model. ROC cusrve is plot of       (1-specificity)      on x axis and sensitivity on y axis.Higher the area under the curve, better is the model.


```{r}
trainingData.clean<- trainingData.normalise[,-c('Age','SkinThickness','Insulin')]
testData.clean<- testData.normalise[,-c('Age','SkinThickness','Insulin')]
glmModel<- glm(Outcome ~.,family=binomial(link="logit"),data=trainingData.clean)
summary(glmModel)
fitted.results <- predict(glmModel,newdata=testData.clean,type='response')
fitted.results.binary <- ifelse(fitted.results > 0.5,1,0)
fitted.results.binary<-as.factor(fitted.results.binary)
conf<- table(testData.clean$Outcome,fitted.results.binary)
conf
acc<- sum(diag(conf))/sum(conf)
acc
fitted.results
testData.clean$Outcome
pred<-prediction(fitted.results,testData.clean$Outcome)
perf<- performance(pred,"tpr","fpr")
plot(perf)


```



19) Even after cleaning data , I can see that the accuracy reamins almost the same.
20) I still decide to go ahead with the clean data and apply other models like kNN and       Random Forest to see how they work.
21) May be LOgistic is just not the right model for this data.
22) I apply knn next with value of k=5. We can change values of k to get a better            accuracy. Keeping K too less will consider very few neighbours and might not give        right model. Keeping k to a very big value will include far off points also and          might not give the right model as well.



```{r}
knn.model<-knn(trainingData.clean,testData.clean,trainingData.clean$Outcome,5)
sum<-summary(knn.model)
knn.conf<- table(testData.clean$Outcome,knn.model)
knn.conf
 acc.knn<- sum(diag(knn.conf))/ sum(knn.conf)
 acc.knn
knn.model
knn.bindf<- rbind(trainingData.clean,testData.clean)
#kdist <- knn.dist(knn.bindf)
??knn.dist

```

23)From the confusion matrix of the above knn model see that  we get an accuracy of        97%  These statistics represent a very good predictive    model as the values are very high.
24) Now we apply Random Forest to predict the values of outcome.



```{r}
randomForest.model<-randomForest(Outcome ~ .,trainingData.clean,importance=TRUE)
randomForest.model
res<-predict(randomForest.model,testData.clean,type="prob")
res
res.probYes<-res[,2]
res.binary <- ifelse(res.probYes > 0.5,1,0)

table.rf<- table(testData.clean$Outcome,res.binary)
table.rf
 acc.rf<- sum(diag(table.rf))/ sum(table.rf)
 acc.rf
 
 pred.rf<-prediction(res.probYes,testData.clean$Outcome)
perf.rs<- performance(pred.rf,"tpr","fpr")
plot(perf.rs)

#confusionMatrix(testData.clean$Outcome,res)
?randomForest


```


25) Random forest gives us an accuracy of 79.6% which is not very good.

26) So we can conclude that kNN is the best model for this which gives us an accuracy of about 96% which is very high.
